{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2952,
     "status": "ok",
     "timestamp": 1762159920551,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "oyDhcmIBaaHB"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1762159922615,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "QAWX2eoL2kvp"
   },
   "outputs": [],
   "source": [
    "from song_code_reproduction.src.cifar_vit import ViT_parameters, ViT, SoftTargetCrossEntropy\n",
    "from song_code_reproduction.src.cifar_aug import (CIFAR10Policy, RandomCropPaste,\n",
    "                                                  MixupCutmix, one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1762159922663,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "JfmFbzz6ja81"
   },
   "outputs": [],
   "source": [
    "save_path = \"/dt/yisroel/Users/Data_Memorization/song_memorization/LSB/cifar/\"\n",
    "\n",
    "# Training Params\n",
    "base_lr_per_256 = 6e-4   # base LR when global batch = 256\n",
    "final_lr = 1e-6\n",
    "warmup_epochs = 15\n",
    "batch_size = 256\n",
    "weight_decay = 0.05\n",
    "num_epochs = 600 # Example number of epochs\n",
    "grad_clip_norm = 1.0\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "global_batch = batch_size\n",
    "base_lr = base_lr_per_256 * (global_batch / 256.0)\n",
    "\n",
    "CIFAR100_MEAN = (0.5071, 0.4867, 0.4408)\n",
    "CIFAR100_STD  = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1799,
     "status": "ok",
     "timestamp": 1762159924464,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "y853H_UcbQSo",
    "outputId": "869f3199-2202-4010-db7b-efe91abba87e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "DataLoaders created successfully!\n",
      "Number of images in training set: 50000\n",
      "Number of images in test set: 10000\n",
      "Number of images in memorization set: 100\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, Subset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations for training with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    CIFAR10Policy(),\n",
    "    RandomCropPaste(size=32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD)\n",
    "])\n",
    "\n",
    "# Define transformations for validation, testing, and memorization set (no augmentation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD)\n",
    "])\n",
    "\n",
    "mem_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t*255).byte().permute(1,2,0).numpy())\n",
    "    \n",
    "])\n",
    "\n",
    "# Download the full CIFAR100 training dataset\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "trainset_mem = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=mem_transform)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=eval_transform)\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=6)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=6)\n",
    "\n",
    "# Create memorization set (100 random images from the original trainset without augmentation)\n",
    "memorization_size = 100\n",
    "# Ensure memorization_size is not greater than the original training set size\n",
    "if memorization_size > len(trainset):\n",
    "    memorization_size = len(trainset)\n",
    "memorization_indices = torch.randperm(len(trainset))[:memorization_size].tolist()\n",
    "memorization_set = Subset(trainset_mem, memorization_indices)\n",
    "\n",
    "\n",
    "print(\"DataLoaders created successfully!\")\n",
    "print(f\"Number of images in training set: {len(trainset)}\")\n",
    "# print(f\"Number of images in validation set: {len(val_dataset)}\")\n",
    "print(f\"Number of images in test set: {len(testset)}\")\n",
    "print(f\"Number of images in memorization set: {len(memorization_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR100\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Lambda()\n",
       "           )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memorization_set.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1762159924740,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "udoiZLHFeET8"
   },
   "outputs": [],
   "source": [
    "model = ViT(**ViT_parameters).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1762159924780,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "f5p8OKdb4F6B"
   },
   "outputs": [],
   "source": [
    "def add_weight_decay(model, weight_decay=0.05, skip_list=()):\n",
    "    decay, no_decay = [], []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if name.endswith(\"bias\") or \"norm\" in name.lower() or name in skip_list or param.dim() == 1:\n",
    "            no_decay.append(param)\n",
    "        else:\n",
    "            decay.append(param)\n",
    "    return [\n",
    "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "param_groups = add_weight_decay(model, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=base_lr, betas=betas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1762159924796,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "qi2Q8h0f3cHs"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = len(trainloader)\n",
    "warmup_steps = warmup_epochs * steps_per_epoch\n",
    "total_steps = num_epochs * steps_per_epoch\n",
    "\n",
    "def cosine_warmup_lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    cosine = 0.5 * (1.0 + math.cos(math.pi * progress))  # 1 -> 0\n",
    "    # map [1..0] to [1..final_lr/base_lr]\n",
    "    return (cosine * (1 - final_lr / base_lr)) + (final_lr / base_lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, cosine_warmup_lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1762159924865,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "9Xbqv9LacHj4",
    "outputId": "41022770-6abc-4ebb-e040-80400c0c1435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training hyperparameters and objects defined.\n",
      "Learning Rate: 0.0006\n",
      "Number of Epochs: 600\n",
      "Loss Function: SoftTargetCrossEntropy\n",
      "Optimizer Type: AdamW\n",
      "Scheduler Type: LambdaLR\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 100\n",
    "mixup_fn = MixupCutmix(num_classes=num_classes, mixup_alpha=0.2, cutmix_alpha=1.0, p=1.0)\n",
    "criterion = SoftTargetCrossEntropy()\n",
    "\n",
    "\n",
    "print(\"Training hyperparameters and objects defined.\")\n",
    "print(f\"Learning Rate: {base_lr}\")\n",
    "print(f\"Number of Epochs: {num_epochs}\")\n",
    "print(f\"Loss Function: {type(criterion).__name__}\")\n",
    "print(f\"Optimizer Type: {type(optimizer).__name__}\")\n",
    "print(f\"Scheduler Type: {type(scheduler).__name__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1762159924870,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "HTtpo-aXe_UL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "def train_batch(images, labels, model, optimizer, criterion,\n",
    "                mixup_fn, device):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    # Optional MixUp/CutMix\n",
    "    if mixup_fn is not None:\n",
    "        images, labels = mixup_fn(images, labels)      # targets are soft probs\n",
    "    else:\n",
    "        labels = labels                                \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_batch(images, labels, model, criterion, device):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, one_hot(labels, num_classes))\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "\n",
    "    return loss.item(), correct, labels.size(0)\n",
    "\n",
    "def main_train_loop(model, trainloader, valloader, optimizer, criterion, mixup_fn,\n",
    "                    scheduler, num_epochs, device, save_path):\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Training phase\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            loss = train_batch(images, labels, model, optimizer, \n",
    "                               criterion, mixup_fn, device)\n",
    "            running_loss += loss\n",
    "\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for images, labels in valloader:\n",
    "            loss, correct, total = evaluate_batch(images, labels, model, criterion, device)\n",
    "            val_loss += loss\n",
    "            correct_predictions += correct\n",
    "            total_predictions += total\n",
    "\n",
    "        epoch_val_loss = val_loss / len(valloader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {epoch_loss:.4f}, '\n",
    "              f'Val Loss: {epoch_val_loss:.4f}, '\n",
    "              f'Val Accuracy: {accuracy:.4f}, '\n",
    "              f'Epoch Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save the best model\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            model_save_path = os.path.join(save_path, 'best_model.pth')\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Saved best model to {model_save_path} with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "        model.train() # Set model back to training mode\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "# To run the training loop, you would call:\n",
    "# main_train_loop(model, trainloader, valloader, optimizer, criterion, scheduler, num_epochs, device, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20627948,
     "status": "ok",
     "timestamp": 1762180552850,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "uguhGoO_fo7g",
    "outputId": "301ef820-bf4f-4aa5-b87d-215c5f1454bf"
   },
   "outputs": [],
   "source": [
    "# main_train_loop(model, trainloader, testloader, optimizer, criterion, mixup_fn,\n",
    "#                     scheduler, num_epochs, device, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fx4awOGAsqt_"
   },
   "source": [
    "## Test model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1762180552945,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "ce4eePUYnI7P",
    "outputId": "12d5c81f-46b6-4e65-ee67-f5dd60c0dedd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /dt/yisroel/Users/Data_Memorization/song_memorization/LSB/cifar/best_model.pth\n",
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the path to the saved model\n",
    "save_path = \"/dt/yisroel/Users/Data_Memorization/song_memorization/LSB/cifar/\"\n",
    "model_save_path = os.path.join(save_path, 'best_model.pth')\n",
    "\n",
    "# Instantiate the model (make sure the model architecture is defined in a previous cell)\n",
    "# Assuming 'model' is already defined and is an instance of your ViT class\n",
    "# model = VisionTransformer(...) # If not already defined, define it here with the correct parameters\n",
    "\n",
    "# Load the saved state dictionary\n",
    "if os.path.exists(model_save_path):\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    print(f\"Model loaded successfully from {model_save_path}\")\n",
    "else:\n",
    "    print(f\"No model found at {model_save_path}\")\n",
    "\n",
    "# Move the model to the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7415,
     "status": "ok",
     "timestamp": 1762180560363,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "ki4NlOIonIgl",
    "outputId": "ea739650-2e9b-4dd5-e8df-1af8a307ba8b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test_model(model, testloader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            loss, correct, total = evaluate_batch(images, labels, model, criterion, device)\n",
    "            test_loss += loss\n",
    "            correct_predictions += correct\n",
    "            total_predictions += total\n",
    "\n",
    "    # Calculate average test loss and accuracy\n",
    "    average_test_loss = test_loss / len(testloader)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    print(f'Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSB encoding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1762180560395,
     "user": {
      "displayName": "Guy Amit",
      "userId": "15828464648338140873"
     },
     "user_tz": -120
    },
    "id": "yZdNZIrJftui"
   },
   "outputs": [],
   "source": [
    "from song_code_reproduction.src.robust_lsb_encoding import (build_tail_payload_from_dataset,\n",
    "                                                            model_capacity_last_byte,\n",
    "                                                            embed_bytes_into_model_last_byte,\n",
    "                                                            extract_all_bytes_from_model_last_byte,\n",
    "                                                            forgiving_tail_parse_from_end)\n",
    "\n",
    "from song_code_reproduction.src.pruning import prune_model_global_l1\n",
    "\n",
    "from song_code_reproduction.src.ssim_eval import ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ssim(memorization_size, recovered_imgs, memorization_set):\n",
    "    ssim_scores = []\n",
    "    for i in range(memorization_size):\n",
    "        with torch.no_grad():\n",
    "            ext_img = torch.tensor(recovered_imgs[i]).permute(2, 0, 1).unsqueeze(0).float()\n",
    "            src_img = torch.tensor(memorization_set[i][0]).permute(2, 0, 1).unsqueeze(0).float()\n",
    "            ssim_score = 0.5*(1+ssim(ext_img, src_img))\n",
    "            ssim_scores.append(ssim_score)\n",
    "    ssim_scores = torch.tensor(ssim_scores)\n",
    "    return ssim_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovered 100 images (no pruning)\n",
      "Test Loss: 1.3457, Test Accuracy: 0.6620\n",
      "SSIM before pruning:  1.0\n",
      "Recovered after pruning: 100\n",
      "SSIM After pruning:  0.5483929514884949\n",
      "Test Loss: 1.3456, Test Accuracy: 0.6575\n"
     ]
    }
   ],
   "source": [
    "# 1) Build payload\n",
    "payload, metas = build_tail_payload_from_dataset(memorization_set, max_images=memorization_size)\n",
    "\n",
    "# 2) Embed AT THE END\n",
    "cap = model_capacity_last_byte(model)\n",
    "if len(payload) > cap: raise RuntimeError(f\"Payload {len(payload)} > cap {cap}\")\n",
    "_ = embed_bytes_into_model_last_byte(model, payload, from_end=True)\n",
    "\n",
    "# 3) Immediate round-trip sanity (no pruning)\n",
    "rb_all = extract_all_bytes_from_model_last_byte(model)\n",
    "assert rb_all[-len(payload):] == payload, \"Tail embed mismatch (should never happen without pruning)\"\n",
    "\n",
    "# 4) Decode from END\n",
    "imgs = forgiving_tail_parse_from_end(rb_all, metas)\n",
    "print(\"Recovered\", len(imgs), \"images (no pruning)\")\n",
    "\n",
    "test_model(model, testloader, device)\n",
    "print(\"SSIM before pruning: \", calc_ssim(memorization_size=memorization_size, recovered_imgs=imgs,\n",
    "                memorization_set=memorization_set).item())\n",
    "# ---- prune the model here ----\n",
    "pruned_model = prune_model_global_l1(model, 0.2)\n",
    "\n",
    "# 5) After pruning, decode from END again (works even if bytes flipped/zeroed)\n",
    "rb_all_after = extract_all_bytes_from_model_last_byte(pruned_model)\n",
    "imgs_after = forgiving_tail_parse_from_end(rb_all_after, metas)\n",
    "print(\"Recovered after pruning:\", len(imgs_after))\n",
    "print(\"SSIM After pruning: \", calc_ssim(memorization_size=memorization_size, recovered_imgs=imgs_after,\n",
    "                memorization_set=memorization_set).item())\n",
    "test_model(pruned_model, testloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
