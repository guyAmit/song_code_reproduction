{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oyDhcmIBaaHB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QAWX2eoL2kvp"
   },
   "outputs": [],
   "source": [
    "from song_code_reproduction.src.mnist_fcn import FCN_parameters, FCModel, LEARNING_RATE, WEIGHT_DECAY\n",
    "# from song_code_reproduction.src.sign_loss import SignEncodingCriterion, reconstruct_bytes_from_model\n",
    "\n",
    "from song_code_reproduction.src.sign_encoding_loss import (\n",
    "    SignEncodingPenalty,\n",
    "    select_all_params,\n",
    "    bytes_to_images_torch,\n",
    "    capacity_report,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JfmFbzz6ja81"
   },
   "outputs": [],
   "source": [
    "save_path = \"/dt/yisroel/Users/Data_Memorization/song_memorization/SIGN/mnist\"\n",
    "\n",
    "memorization_size = 100\n",
    "\n",
    "# Training Params\n",
    "BATCH_SIZE = 128\n",
    "num_epochs = 100 # Example number of epochs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y853H_UcbQSo",
    "outputId": "03aa4300-a272-4a41-83ee-37b00ca8ea88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created successfully!\n",
      "Number of images in training set: 48000\n",
      "Number of images in validation set: 12000\n",
      "Number of images in test set: 10000\n",
      "Number of images in memorization set: 100\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for training with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define transformations for validation, testing, and memorization set (no augmentation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mem_transform = transforms.Compose([])\n",
    "\n",
    "\n",
    "# Download the full CIFAR100 training dataset\n",
    "full_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=eval_transform)\n",
    "\n",
    "\n",
    "# Split the full training set into training and validation indices\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "train_indices, val_indices = random_split(range(len(full_trainset)), [train_size, val_size])\n",
    "\n",
    "# Create training and validation datasets using indices and applying appropriate transforms\n",
    "train_dataset = Subset(full_trainset, train_indices.indices)\n",
    "val_dataset = Subset(full_trainset, val_indices.indices)\n",
    "\n",
    "# Apply transforms to the datasets\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform = eval_transform\n",
    "\n",
    "if memorization_size > len(full_trainset):\n",
    "    memorization_size = len(full_trainset)\n",
    "memorization_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "\n",
    "memorization_indices = torch.randperm(len(full_trainset))[:memorization_size].tolist()\n",
    "memorization_set = Subset(memorization_set, memorization_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "print(\"DataLoaders created successfully!\")\n",
    "print(f\"Number of images in training set: {len(train_dataset)}\")\n",
    "print(f\"Number of images in validation set: {len(val_dataset)}\")\n",
    "print(f\"Number of images in test set: {len(testset)}\")\n",
    "print(f\"Number of images in memorization set: {len(memorization_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "udoiZLHFeET8"
   },
   "outputs": [],
   "source": [
    "model = FCModel(**FCN_parameters).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Xbqv9LacHj4",
    "outputId": "1d6c54ca-602d-478c-9825-b40eba28c427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capacity: subset L=3962890, redundancy k=5, available bits B=627200, fits ~100 images\n",
      "Training hyperparameters and objects defined.\n",
      "Learning Rate: 0.0001\n",
      "Number of Epochs: 100\n",
      "Loss Function: CrossEntropyLoss\n",
      "Optimizer Type: AdamW\n",
      "Scheduler Type: CosineAnnealingLR\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "sign_penalty = SignEncodingPenalty(\n",
    "    model=model,\n",
    "    dataset=memorization_set,\n",
    "    subset_selector=select_all_params,\n",
    "    lambda_max=50.0,\n",
    "    margin=1e-3,\n",
    "    redundancy_k=5,         # IMPORTANT: maximize capacity for visible recon\n",
    "    device=device,\n",
    "    float_mode=\"clip01\",\n",
    ")\n",
    "print(capacity_report(sign_penalty, bits_per_image=28*28*8))\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr= 5e-4, weight_decay=0)\n",
    "\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "print(\"Training hyperparameters and objects defined.\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of Epochs: {num_epochs}\")\n",
    "print(f\"Loss Function: {type(criterion).__name__}\")\n",
    "print(f\"Optimizer Type: {type(optimizer).__name__}\")\n",
    "print(f\"Scheduler Type: {type(scheduler).__name__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HTtpo-aXe_UL"
   },
   "outputs": [],
   "source": [
    "def train_batch(images, labels, model, optimizer, criterion, step, total_steps, device):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    sign_pen = sign_penalty(step, total_steps)\n",
    "    # print(sign_pen)\n",
    "    loss = criterion(outputs, labels) + sign_pen\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_batch(images, labels, model, criterion, device):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "      # Forward pass\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # Calculate accuracy\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      correct = (predicted == labels).sum().item()\n",
    "\n",
    "    return loss.item(), correct, labels.size(0)\n",
    "\n",
    "def main_train_loop(model, trainloader, valloader, optimizer, criterion, scheduler, num_epochs, device, save_path):\n",
    "    best_train_loss = float('inf')\n",
    "\n",
    "    model.train()\n",
    "    total_steps = num_epochs * len(trainloader)\n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Training phase\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            loss = train_batch(images, labels, model, optimizer, criterion, step, total_steps, device)\n",
    "            running_loss += loss\n",
    "            step += 1\n",
    "\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for images, labels in valloader:\n",
    "            loss, correct, total = evaluate_batch(images, labels, model, criterion, device)\n",
    "            val_loss += loss\n",
    "            correct_predictions += correct\n",
    "            total_predictions += total\n",
    "\n",
    "        epoch_val_loss = val_loss / len(valloader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {epoch_loss:.4f}, '\n",
    "              f'Val Loss: {epoch_val_loss:.4f}, '\n",
    "              f'Val Accuracy: {accuracy:.4f}, '\n",
    "              f'Epoch Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save the model with lowest training loss\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "            model_save_path = os.path.join(save_path, 'best_model.pth')\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Saved best model to {model_save_path} with training loss: {best_train_loss:.4f}\")\n",
    "\n",
    "        model.train() # Set model back to training mode\n",
    "        model_save_path = os.path.join(save_path, 'last_model.pth')\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uguhGoO_fo7g",
    "outputId": "82ab0654-608b-4417-beff-ad31bfd5586f"
   },
   "outputs": [],
   "source": [
    "# main_train_loop(model, trainloader, valloader, optimizer, criterion, scheduler, num_epochs, device, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fx4awOGAsqt_"
   },
   "source": [
    "## Test model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce4eePUYnI7P",
    "outputId": "123cd7c6-e595-45a7-c52b-12c3d119e44d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /dt/yisroel/Users/Data_Memorization/song_memorization/SIGN/mnist/last_model.pth\n",
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the path to the saved model\n",
    "save_path = \"/dt/yisroel/Users/Data_Memorization/song_memorization/SIGN/mnist\"\n",
    "model_save_path = os.path.join(save_path, 'last_model.pth')\n",
    "\n",
    "# Instantiate the model (make sure the model architecture is defined in a previous cell)\n",
    "# Assuming 'model' is already defined and is an instance of your ViT class\n",
    "# model = VisionTransformer(...) # If not already defined, define it here with the correct parameters\n",
    "\n",
    "# Load the saved state dictionary\n",
    "if os.path.exists(model_save_path):\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    print(f\"Model loaded successfully from {model_save_path}\")\n",
    "else:\n",
    "    print(f\"No model found at {model_save_path}\")\n",
    "\n",
    "# Move the model to the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ki4NlOIonIgl",
    "outputId": "5f721299-d234-4775-855c-6a5d9e34b78e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test_model(model, testloader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            loss, correct, total = evaluate_batch(images, labels, model, criterion, device)\n",
    "            test_loss += loss\n",
    "            correct_predictions += correct\n",
    "            total_predictions += total\n",
    "\n",
    "    # Calculate average test loss and accuracy\n",
    "    average_test_loss = test_loss / len(testloader)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    print(f'Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyNO4iHariL1"
   },
   "source": [
    "## SIGN encoding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "SeNE_Gv0shW_"
   },
   "outputs": [],
   "source": [
    "from song_code_reproduction.src.ssim_eval import ssim\n",
    "from song_code_reproduction.src.pruning import prune_model_global_l1\n",
    "from song_code_reproduction.src.sign_encoding_loss import (select_all_params, bytes_to_images_torch,\n",
    "                                                           decode_bytes_from_given_model, bytes_to_images_torch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "zEn_5tiNhvEQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded bytes: 78400 → can reconstruct up to 100 images\n",
      "Reconstructed tensor: torch.Size([100, 1, 28, 28]) torch.uint8 0 255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----- Decode images -----\n",
    "decoded = sign_penalty.decode_bytes_from_model()\n",
    "\n",
    "H, W, C, order = 28, 28, 1, \"CHW\"\n",
    "per_image_bytes = H * W * C  # 784\n",
    "max_images = len(decoded) // per_image_bytes\n",
    "print(f\"Decoded bytes: {len(decoded)} → can reconstruct up to {max_images} images\")\n",
    "\n",
    "N = min(memorization_size, max_images)\n",
    "if N == 0:\n",
    "    print(\"Not enough capacity for a full image. Increase subset size or reduce redundancy.\")\n",
    "else:\n",
    "    imgs = bytes_to_images_torch(decoded, n=N, h=H, w=W, c=C, order=order, device=\"cpu\")\n",
    "    print(\"Reconstructed tensor:\", imgs.shape, imgs.dtype, imgs.min().item(), imgs.max().item())\n",
    "    # Optionally visualize\n",
    "    import matplotlib.pyplot as plt\n",
    "    grid = torch.cat([imgs[i] for i in range(min(16, N))], dim=2)[0]\n",
    "    plt.imshow(grid.numpy(), cmap=\"gray\")\n",
    "    plt.title(\"Decoded (concatenated) samples\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def decode(model, sign_penalty, size = (28, 28, 1), order=\"CHW\"):\n",
    "    \n",
    "    subset_selector = select_all_params     # or select_linear_head_params — must match training\n",
    "    k = sign_penalty.k\n",
    "    B_bits = int(sign_penalty.B_bits.item())\n",
    "    \n",
    "    decoded = decode_bytes_from_given_model(\n",
    "        model=model,\n",
    "        subset_selector=subset_selector,\n",
    "        k=k,\n",
    "        B_bits=B_bits,\n",
    "    )\n",
    "    \n",
    "    # 3) Turn bytes into images (MNIST shapes)\n",
    "    H, W, C, order = 28, 28, 1, \"CHW\"\n",
    "    per_image_bytes = H * W * C\n",
    "\n",
    "    max_images = len(decoded) // per_image_bytes\n",
    "\n",
    "    imgs = bytes_to_images_torch(decoded, n=max_images, h=H, w=W, c=C, order=order, device=\"cpu\")\n",
    "    return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "yZdNZIrJftui",
    "outputId": "5bf13eb8-b66b-4449-c2c3-e732578c5582"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.matshow((imgs[4].reshape(28,28)), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5uCVJoxE8b_",
    "outputId": "36b4eb93-ec16-4f9a-88f3-836277f78f5f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "K7bEFAr0tLUH"
   },
   "outputs": [],
   "source": [
    "def calc_ssim(memorization_set, recon, memorization_size):\n",
    "    ssim_scores = []\n",
    "    for i in range(len(recon)):\n",
    "        with torch.no_grad():\n",
    "            ext_img = recon[i].unsqueeze(0).to(torch.float32)\n",
    "            src_img = torch.tensor(np.array(memorization_set[i][0])).view(1, 1, 28, 28).to(torch.float32)\n",
    "            ssim_score = 0.5*(ssim(ext_img, src_img)+1)\n",
    "            ssim_scores.append(ssim_score)\n",
    "    ssim_scores = torch.tensor(ssim_scores)\n",
    "    return ssim_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zeWS5y46tO-h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning:\n",
      "SSIM: 0.9892\n",
      "Test Loss: 0.1114, Test Accuracy: 0.9796\n",
      "\n",
      "After pruning:\n",
      "SSIM: 0.5385\n",
      "Test Loss: 0.1151, Test Accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "imgs = decode(model, sign_penalty)\n",
    "ssim_before_pruning = calc_ssim(memorization_set, imgs, memorization_size)\n",
    "print(\"Before pruning:\")\n",
    "print(f\"SSIM: {ssim_before_pruning:.4f}\")\n",
    "test_model(model, testloader, device)\n",
    "\n",
    "pruned_model = prune_model_global_l1(model, 0.2)\n",
    "print()\n",
    "\n",
    "print(\"After pruning:\")\n",
    "imgs = decode(pruned_model, sign_penalty)\n",
    "ssim_after_pruning = calc_ssim(memorization_set, imgs, memorization_size)\n",
    "print(f\"SSIM: {ssim_after_pruning:.4f}\")\n",
    "test_model(pruned_model, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
